import os

# Start up a Redis instance
r = redis.StrictRedis(host='localhost', port=6379, db=0)

# Pull out all the recommendations from HDFS
p = os.popen("hadoop fs -cat recommendations/part*")

# Load the recommendations into Redis
for i in p:

  # Split recommendations into key of user id
  # and value of recommendations
  # E.g., 35^I[2067:5.0,17:5.0,1041:5.0,2068:5.0,2087:5.0,
  #       1036:5.0,900:5.0,1:5.0,081:5.0,3135:5.0]$
  k,v = i.split('\t')

  # Put key, value into Redis
  r.set(k,v)

# Establish an endpoint that takes in user id in the path
@route('/<string:id>')

def recs(request, id):
  # Get recommendations for this user
  v = r.get(id)
  return 'The recommendations for user '+id+' are '+v


# Make a default endpoint
@route('/')

def home(request):
  return 'Please add a user id to the URL, e.g. http://localhost:8081/1234n'

# Start up a listener on port 8080
run("localhost", 8081)[hadoop@ip-172-31-19-126 ~]$ twistd -noy hello.py &
[3] 23996
[hadoop@ip-172-31-19-126 ~]$ 2020-02-29 16:00:53+0000 [-] Log opened.
2020-02-29 16:00:53+0000 [-] Site starting on 8081
2020-02-29 16:00:53+0000 [-] Starting factory <twisted.web.server.Site instance at 0x7fb9706d2638>

[hadoop@ip-172-31-19-126 ~]$ curl localhost:8081/37
2020-02-29 16:03:52+0000 [-] "127.0.0.1" - - [29/Feb/2020:16:03:51 +0000] "GET /37 HTTP/1.1" 200 125 "-" "curl/7.61.1"
The recommendations for user 37 are [265:5.0,1449:5.0,3100:5.0,3035:5.0,2243:5.0,3168:5.0,1517:5.0,594:5.0,1188:5.0,728:5.0]
[hadoop@ip-172-31-19-126 ~]$ [21013] 29 Feb 16:04:50.034 * 100 changes in 300 seconds. Saving...
[21013] 29 Feb 16:04:50.035 * Background saving started by pid 26386
[26386] 29 Feb 16:04:50.045 * DB saved on disk
[26386] 29 Feb 16:04:50.046 * RDB: 0 MB of memory used by copy-on-write
[21013] 29 Feb 16:04:50.135 * Background saving terminated with success

[hadoop@ip-172-31-19-126 ~]$
[hadoop@ip-172-31-19-126 ~]$
[hadoop@ip-172-31-19-126 ~]$
[hadoop@ip-172-31-19-126 ~]$
[hadoop@ip-172-31-19-126 ~]$ ls -lrt
total 18116
-rw-rw-r-- 1 hadoop hadoop  1064262 Mar  5  2014 redis-2.8.7.tar.gz
drwxr-x--- 2 hadoop hadoop     4096 Jan 29  2016 ml-1m
-rw-rw-r-- 1 hadoop hadoop  5917549 Dec  3 17:14 ml-1m.zip
-rw-rw-r-- 1 hadoop hadoop 11553456 Feb 29 15:45 ratings.csv
-rw-rw-r-- 1 hadoop hadoop      955 Feb 29 16:00 hello.py
drwxrwxr-x 6 hadoop hadoop     4096 Feb 29 16:04 redis-2.8.7
[hadoop@ip-172-31-19-126 ~]$ rm hello.py
[hadoop@ip-172-31-19-126 ~]$ rm ratings.csv
[hadoop@ip-172-31-19-126 ~]$ ls -lrt
total 6828
-rw-rw-r-- 1 hadoop hadoop 1064262 Mar  5  2014 redis-2.8.7.tar.gz
drwxr-x--- 2 hadoop hadoop    4096 Jan 29  2016 ml-1m
-rw-rw-r-- 1 hadoop hadoop 5917549 Dec  3 17:14 ml-1m.zip
drwxrwxr-x 6 hadoop hadoop    4096 Feb 29 16:04 redis-2.8.7
[hadoop@ip-172-31-19-126 ~]$ rm ml-1m.zip
[hadoop@ip-172-31-19-126 ~]$ rm ml-1m
rm: cannot remove ‘ml-1m’: Is a directory
[hadoop@ip-172-31-19-126 ~]$ ls -lrt
total 1048
-rw-rw-r-- 1 hadoop hadoop 1064262 Mar  5  2014 redis-2.8.7.tar.gz
drwxr-x--- 2 hadoop hadoop    4096 Jan 29  2016 ml-1m
drwxrwxr-x 6 hadoop hadoop    4096 Feb 29 16:04 redis-2.8.7
[hadoop@ip-172-31-19-126 ~]$ clear
[hadoop@ip-172-31-19-126 ~]$ ls -lrt
total 1048
-rw-rw-r-- 1 hadoop hadoop 1064262 Mar  5  2014 redis-2.8.7.tar.gz
drwxr-x--- 2 hadoop hadoop    4096 Jan 29  2016 ml-1m
drwxrwxr-x 6 hadoop hadoop    4096 Feb 29 16:04 redis-2.8.7
[hadoop@ip-172-31-19-126 ~]$ rm ml-1m
rm: cannot remove ‘ml-1m’: Is a directory
[hadoop@ip-172-31-19-126 ~]$ rm -r ml-1m
[hadoop@ip-172-31-19-126 ~]$ ls -lrt
total 1044
-rw-rw-r-- 1 hadoop hadoop 1064262 Mar  5  2014 redis-2.8.7.tar.gz
drwxrwxr-x 6 hadoop hadoop    4096 Feb 29 16:04 redis-2.8.7
[hadoop@ip-172-31-19-126 ~]$ clear
[hadoop@ip-172-31-19-126 ~]$ wget http://files.grouplens.org/datasets/movielens/ml-1m.zip
--2020-02-29 16:12:40--  http://files.grouplens.org/datasets/movielens/ml-1m.zip
Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152
Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 5917549 (5.6M) [application/zip]
Saving to: ‘ml-1m.zip’

ml-1m.zip                                 100%[=====================================================================================>]   5.64M  23.5MB/s    in 0.2s

2020-02-29 16:12:41 (23.5 MB/s) - ‘ml-1m.zip’ saved [5917549/5917549]

[hadoop@ip-172-31-19-126 ~]$ unzip ml-1m.zip
Archive:  ml-1m.zip
   creating: ml-1m/
  inflating: ml-1m/movies.dat
  inflating: ml-1m/ratings.dat
  inflating: ml-1m/README
  inflating: ml-1m/users.dat
[hadoop@ip-172-31-19-126 ~]$ ls -lrt
total 6828
-rw-rw-r-- 1 hadoop hadoop 1064262 Mar  5  2014 redis-2.8.7.tar.gz
drwxr-x--- 2 hadoop hadoop    4096 Jan 29  2016 ml-1m
-rw-rw-r-- 1 hadoop hadoop 5917549 Dec  3 17:14 ml-1m.zip
drwxrwxr-x 6 hadoop hadoop    4096 Feb 29 16:04 redis-2.8.7
[hadoop@ip-172-31-19-126 ~]$ cat ml-1m/ratings.dat | sed 's/::/,/g' | cut -f1-3 -d, > ratings.csv
[hadoop@ip-172-31-19-126 ~]$ hadoop fs -put ratings.csv /ratings.csv
put: `/ratings.csv': File exists
[hadoop@ip-172-31-19-126 ~]$ mahout recommenditembased --input /ratings.csv --output recommendations --numRecommendations 10 --outputPathForSimilarityMatrix similarity-matrix --similarityClassname SIMILARITY_COSINE
MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.
Running on hadoop, using /usr/lib/hadoop/bin/hadoop and HADOOP_CONF_DIR=/etc/hadoop/conf
MAHOUT-JOB: /usr/lib/mahout/mahout-examples-0.13.0-job.jar
20/02/29 16:13:51 INFO AbstractJob: Command line arguments: {--booleanData=[false], --endPhase=[2147483647], --input=[/ratings.csv], --maxPrefsInItemSimilarity=[500], --maxPrefsPerUser=[10], --maxSimilaritiesPerItem=[100], --minPrefsPerUser=[1], --numRecommendations=[10], --output=[recommendations], --outputPathForSimilarityMatrix=[similarity-matrix], --similarityClassname=[SIMILARITY_COSINE], --startPhase=[0], --tempDir=[temp]}
20/02/29 16:13:51 INFO AbstractJob: Command line arguments: {--booleanData=[false], --endPhase=[2147483647], --input=[/ratings.csv], --minPrefsPerUser=[1], --output=[temp/preparePreferenceMatrix], --ratingShift=[0.0], --startPhase=[0], --tempDir=[temp]}
20/02/29 16:13:51 INFO deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir
20/02/29 16:13:51 INFO deprecation: mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress
20/02/29 16:13:51 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
20/02/29 16:13:51 INFO RMProxy: Connecting to ResourceManager at ip-172-31-19-126.ec2.internal/172.31.19.126:8032
Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory temp/preparePreferenceMatrix/itemIDIndex already exists
        at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:146)
        at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:268)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:141)
        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1341)
        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1338)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1338)
        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1359)
        at org.apache.mahout.cf.taste.hadoop.preparation.PreparePreferenceMatrixJob.run(PreparePreferenceMatrixJob.java:77)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.run(RecommenderJob.java:168)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.main(RecommenderJob.java:335)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:152)
        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:239)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:153)
[hadoop@ip-172-31-19-126 ~]$  hadoop fs -ls recommendations
Found 4 items
-rw-r--r--   1 hadoop hadoop          0 2020-02-29 15:50 recommendations/_SUCCESS
-rw-r--r--   1 hadoop hadoop     196962 2020-02-29 15:50 recommendations/part-r-00000
-rw-r--r--   1 hadoop hadoop     197744 2020-02-29 15:50 recommendations/part-r-00001
-rw-r--r--   1 hadoop hadoop     197600 2020-02-29 15:50 recommendations/part-r-00002
[hadoop@ip-172-31-19-126 ~]$ hadoop fs -cat recommendations/part-r-00000 | head
3       [2375:5.0,3168:5.0,1517:5.0,2376:5.0,2243:5.0,1320:5.0,198:5.0,594:5.0,3035:5.0,3494:5.0]
6       [3035:5.0,2375:5.0,594:5.0,2572:5.0,2110:5.0,3034:5.0,3298:5.0,3826:5.0,3100:5.0,2243:5.0]
9       [2112:5.0,196:5.0,1517:5.0,1912:5.0,1320:5.0,1449:5.0,198:5.0,3100:5.0,265:5.0,1120:5.0]
12      [922:5.0,3033:5.0,592:5.0,928:5.0,596:5.0,2243:5.0,930:5.0,1517:5.0,594:5.0,3552:5.0]
15      [3566:4.5834103,3745:4.5380926,3555:4.5304275,3354:4.526979,3895:4.5090156,3189:4.494031,3793:4.4882627,3784:4.477075,3593:4.470243,1270:4.4702353]
18      [2112:5.0,1320:5.0,3035:5.0,1584:5.0,2376:5.0,2243:5.0,3430:5.0,2110:5.0,1187:5.0,3168:5.0]
21      [778:5.0,1356:5.0,3785:5.0,3863:5.0,3617:5.0,3752:5.0,3948:5.0,3624:5.0,3535:5.0,3793:4.460076]
24      [529:5.0,2375:5.0,3035:5.0,265:5.0,2376:5.0,2374:5.0,2110:5.0,1188:5.0,1584:5.0,2243:5.0]
27      [3629:5.0,2243:5.0,3035:5.0,2968:5.0,1912:5.0,2967:5.0,198:5.0,1253:5.0,1188:5.0,3168:5.0]
30      [1294:5.0,1258:5.0,2871:4.8277783,2144:4.7735796,1208:4.7728972,1748:4.7581077,3499:4.7446394,1302:4.7331414,1704:4.726853,904:4.7040987]
cat: Unable to write to output stream.
[hadoop@ip-172-31-19-126 ~]$ sudo easy_install twisted
Searching for twisted
Best match: Twisted 19.10.0
Processing Twisted-19.10.0-py2.7-linux-x86_64.egg
Twisted 19.10.0 is already the active version in easy-install.pth
Installing trial script to /usr/local/bin
Installing conch script to /usr/local/bin
Installing ckeygen script to /usr/local/bin
Installing twist script to /usr/local/bin
Installing pyhtmlizer script to /usr/local/bin
Installing mailmail script to /usr/local/bin
Installing tkconch script to /usr/local/bin
Installing twistd script to /usr/local/bin
Installing cftp script to /usr/local/bin

Using /usr/local/lib/python2.7/site-packages/Twisted-19.10.0-py2.7-linux-x86_64.egg
Processing dependencies for twisted
Finished processing dependencies for twisted
[hadoop@ip-172-31-19-126 ~]$ sudo easy_install klein
Searching for klein
Best match: klein 19.6.0
Processing klein-19.6.0-py2.7.egg
klein 19.6.0 is already the active version in easy-install.pth

Using /usr/local/lib/python2.7/site-packages/klein-19.6.0-py2.7.egg
Processing dependencies for klein
Finished processing dependencies for klein
[hadoop@ip-172-31-19-126 ~]$ sudo easy_install redis
Searching for redis
Best match: redis 3.4.1
Processing redis-3.4.1-py2.7.egg
redis 3.4.1 is already the active version in easy-install.pth

Using /usr/local/lib/python2.7/site-packages/redis-3.4.1-py2.7.egg
Processing dependencies for redis
Finished processing dependencies for redis
[hadoop@ip-172-31-19-126 ~]$ ls -lrt
total 18112
-rw-rw-r-- 1 hadoop hadoop  1064262 Mar  5  2014 redis-2.8.7.tar.gz
drwxr-x--- 2 hadoop hadoop     4096 Jan 29  2016 ml-1m
-rw-rw-r-- 1 hadoop hadoop  5917549 Dec  3 17:14 ml-1m.zip
drwxrwxr-x 6 hadoop hadoop     4096 Feb 29 16:04 redis-2.8.7
-rw-rw-r-- 1 hadoop hadoop 11553456 Feb 29 16:13 ratings.csv
[hadoop@ip-172-31-19-126 ~]$ ls -lrt
total 18112
-rw-rw-r-- 1 hadoop hadoop  1064262 Mar  5  2014 redis-2.8.7.tar.gz
drwxr-x--- 2 hadoop hadoop     4096 Jan 29  2016 ml-1m
-rw-rw-r-- 1 hadoop hadoop  5917549 Dec  3 17:14 ml-1m.zip
drwxrwxr-x 6 hadoop hadoop     4096 Feb 29 16:04 redis-2.8.7
-rw-rw-r-- 1 hadoop hadoop 11553456 Feb 29 16:13 ratings.csv
[hadoop@ip-172-31-19-126 ~]$ cat > hello.py
from klein import run, route
import redis
import os

# Start up a Redis instance
r = redis.StrictRedis(host='localhost', port=6379, db=0)

# Pull out all the recommendations from HDFS
p = os.popen("hadoop fs -cat recommendations/part*")

# Load the recommendations into Redis
for i in p:

  # Split recommendations into key of user id
  # and value of recommendations
  # E.g., 35^I[2067:5.0,17:5.0,1041:5.0,2068:5.0,2087:5.0,
  #       1036:5.0,900:5.0,1:5.0,081:5.0,3135:5.0]$
  k,v = i.split('\t')

  # Put key, value into Redis
  r.set(k,v)

# Establish an endpoint that takes in user id in the path
@route('/<string:id>')

def recs(request, id):
  # Get recommendations for this user
  v = r.get(id)
  return 'The recommendations for user '+id+' are '+v


# Make a default endpoint
@route('/')

def home(request):
  return 'Please add a user id to the URL, e.g. http://localhost:8081/1234n'

# Start up a listener on port 8080
run("localhost", 8081)[hadoop@ip-172-31-19-126 ~]$ twistd -noy hello.py &
[4] 30297
[hadoop@ip-172-31-19-126 ~]$ [21013] 29 Feb 16:16:08.541 * 100 changes in 300 seconds. Saving...
[21013] 29 Feb 16:16:08.541 * Background saving started by pid 30338
[30338] 29 Feb 16:16:08.551 * DB saved on disk
[30338] 29 Feb 16:16:08.551 * RDB: 0 MB of memory used by copy-on-write
[21013] 29 Feb 16:16:08.641 * Background saving terminated with success
2020-02-29 16:16:09+0000 [-] Log opened.
2020-02-29 16:16:09+0000 [-] Unhandled error in Deferred:
2020-02-29 16:16:09+0000 [-] Unhandled Error
        Traceback (most recent call last):
          File "/usr/local/lib/python2.7/site-packages/Twisted-19.10.0-py2.7-linux-x86_64.egg/twisted/persisted/sob.py", line 177, in loadValueFromFile
            eval(codeObj, d, d)
          File "hello.py", line 39, in <module>
            run("localhost", 8081)
          File "/usr/local/lib/python2.7/site-packages/klein-19.6.0-py2.7.egg/klein/_app.py", line 437, in run
            endpoint.listen(site)
          File "/usr/local/lib/python2.7/site-packages/Twisted-19.10.0-py2.7-linux-x86_64.egg/twisted/internet/endpoints.py", line 495, in listen
            interface=self._interface)
        --- <exception caught here> ---
          File "/usr/local/lib/python2.7/site-packages/Twisted-19.10.0-py2.7-linux-x86_64.egg/twisted/internet/defer.py", line 122, in execute
            result = callable(*args, **kw)
          File "/usr/local/lib/python2.7/site-packages/Twisted-19.10.0-py2.7-linux-x86_64.egg/twisted/internet/posixbase.py", line 495, in listenTCP
            p.startListening()
          File "/usr/local/lib/python2.7/site-packages/Twisted-19.10.0-py2.7-linux-x86_64.egg/twisted/internet/tcp.py", line 1363, in startListening
            raise CannotListenError(self.interface, self.port, le)
        twisted.internet.error.CannotListenError: Couldn't listen on localhost:8081: [Errno 98] Address already in use.


[hadoop@ip-172-31-19-126 ~]$
[hadoop@ip-172-31-19-126 ~]$ curl localhost:8081/37
2020-02-29 16:18:19+0000 [-] "127.0.0.1" - - [29/Feb/2020:16:18:18 +0000] "GET /37 HTTP/1.1" 200 125 "-" "curl/7.61.1"
The recommendations for user 37 are [265:5.0,1449:5.0,3100:5.0,3035:5.0,2243:5.0,3168:5.0,1517:5.0,594:5.0,1188:5.0,728:5.0]
[hadoop@ip-172-31-19-126 ~]$ [21013] 29 Feb 16:21:09.003 * 100 changes in 300 seconds. Saving...
[21013] 29 Feb 16:21:09.003 * Background saving started by pid 440
[440] 29 Feb 16:21:09.019 * DB saved on disk
[440] 29 Feb 16:21:09.019 * RDB: 0 MB of memory used by copy-on-write
[21013] 29 Feb 16:21:09.104 * Background saving terminated with success
